#' @title Model and parameter simultaneous tuning
#'
#' @description 
#' Tuning multiple models and hyper-parameter using grid search or randomized search, in a optional parallel fashion. 
#
#' @param x                 Design matrix, usually derived from model.matrix.
#' @param y                 Response vector, numeric (regression), factor (classification) or 'Surv' object
#' @param formula           A model formula simliar to formula in \code{\link{lm}}
#' @param data              A data frame contains for the training data
#' @param weights           Sample weigth
#' @param models            A vector of names from \code{models} in models.RData
#' @param modelControl      A list of named (same as in \code{models}) lists contains additional parameters (cannot be tuning paramters)
#' 							If one needs the arguments to be evaluated when inside the inner tuning loop, it should be quoted.
#' 							For example,  to use a balanced randomForest, one should would specify it like \code{sampsize = quote(rep(min(table(y)), 2))},
#' 							where the y will be evaluated in the resampling loop, instead of being evaluated in the environment where mpTune is called
#' @param preProcess        A preProcess object from \code{caret}
#' @param gridLength        Grid length if using grid search for hyper-parameters (tuning parameters)
#' @param randomizedLength  Number of hyper-parameter configuration if randomizedSearch is available for models
#' @param mpTnControl       A list generated by function \code{\link{mpTuneControl}} 
#' @param loopingRule       A function that actually does the iteration, looked like function(executeFunciton, loopList, ...), similar to base::Map.
#'                          One can use it to pass a customized parallel method. Built-in choice are foreachLoop (default), mclapplyLoop, mclapplyBatchLoop,
#'                          parLapplyLoop and parLapplyLBLoop (balanced load parallel::paraLapply), parLapplyBLBatchLoop, and the non-paralled lapplyLoop.
#'                          See ?loopingRule for detail on these functions
#' @param verbose           Should fitting message be displayed if any
#' @param ...               For mpTune.formula, this is the arguments passed to muTune.matrix
#'                          For mpTune.mpTune, it is 'modelControl', 'verbose', 'test'
#' 
#' @return a list with class 'mpTune' or 'mpTune.formula' (inherits of 'mpTune') containing the following entries \cr
#'         $ allModelsPerformance: a list of lists (length = number of models) of all tried model-parameter combinations, not ranked \cr
#'         $ allCVs: a list of lists of all tried model-parameter-fold combinations, not ranked \cr
#'         $ sampleIndex: a list of cross validation folds. Each fold is a validation or test set, and its complementary set is the training \cr
#'         $ data: list(x, y, weights) or list(formula, data, weights) mpTnControl$returnData if TRUE (default) \cr
#'         $ performanceMetric: performance matrix used, which is specified in mpTune$summaryFunction \cr
#'         $ config: a list of \cr
#'             $ sampleIndex: a list of resample used for tuning  \cr
#'             $ models: a list of models as specified \cr
#'             $ modelControl: a list of further arguments as specifed  \cr
#'             $ mpTnControl: a 'mpTuneControl' object as specifed \cr
#'             $ preProcess: a 'preProcess' object as specified \cr
#'             $ gridLength: a vector of gridLength as specifed \cr
#'             $ randomizedLength: a vector of randomized research as specifed \cr
#'
#' @examples
#' \dontrun{
#' # turn on the  following two statements if a multicore parallel is requested.
#' # library(doMC);
#' # registerDoMC(cores = detectCores());
#' 
#' if (require(mlbench)) {
#'     data(Sonar, package = 'mlbench');
#'     inTraining <- sample(1:nrow(Sonar), floor(nrow(Sonar)*0.6), replace = TRUE); 
#'     training   <- Sonar[inTraining, ];
#'     testing    <- Sonar[-inTraining, ];
#' 
#'     x <- model.matrix(Class ~ ., training)[, -1];
#'     y <- training$Class;
#' 
#'     sonarTuned <- mpTune(
#'         formula = Class ~. ,
#'         data = training,
#'         models =  list(balancedRF = 'rf', rf = 'rf', 'gbm'), 
#'         mpTnControl = mpTuneControl(
#'             samplingFunction = createCVFolds, nfold = 3, repeats = 1, 
#'             stratify = TRUE, classProbs = TRUE,
#'             summaryFunction = requireSummary(metric = c('AUC', 'BAC', 'Kappa'))),
#'         gridLength = 3, 
#'         randomizedLength = 3, 
#'         modelControl = list(
#'             gbm = list(verbose = FALSE),
#'             balancedRF = list(ntree = 100, sampsize = quote(rep(min(table(y)), 2)))
#'             )
#'         );
#' 
#'     print(sonarTuned)
#'     summary(sonarTuned)
#' 
#' # tune more model
#'     sonarTuned <- more(sonarTuned, models = 'glmnet');
#'      
#' # Now sonarTuned contains tuning information of four models: balancedRF, rf, gbm and glmnet
#' # fit the model giving the best 'AUC' 
#'     bestModel <- fit(sonarTuned, metric = 'AUC')
#'     bestModel
#' 
#' # predict on hold out sample
#' # sonarTestPred <- predict(bestModel, newdata = test);
#' 
#' # perform a cross validation for a fair performance estimate cosidering multiple model tunings and selections
#'     sonarTunedPerf <- resample(sonarTuned, nfold = 3, repeats = 1, stratify = TRUE);
#'     sonarTunedPerf
#'     }
#' 
#' ## 
#' ## Survival analysis
#' ##
#' 
#' # check what models are avaible for right censored survival data
#' getDefaultModel(10, type = 'survival')
#' 
#' 
#' if (require(randomForestSRC)) {
#'     data(pbc, package = 'randomForestSRC');
#'     pbc <- na.omit(pbc);
#' 
#' 
#'     survTune <- mpTune(
#'         Surv(days, status) ~., 
#'         data = pbc,
#'         models = list(
#'             Cox = 'coxph',
#'             elasticnet = 'glmnet',
#'             gbm = 'gbm',
#'             survivalForest = 'rfsrc',
#'             boostedCox = 'blackboost',
#'             boostedSCI = 'glmboost'
#'             ),
#'         mpTnControl = mpTuneControl(
#'             samplingFunction = createCVFolds,nfold = 3, repeats = 1, 
#'             stratify = TRUE, summaryFunction = survivalSummary),
#'         modelControl = list(
#'             boostedSCI = list(family = SCI()),
#'             gbm = list(verbose = FALSE)
#'             ),
#'         gridLength = 2, 
#'         randomizedLength = 3
#'         );
#'
#'     print(survTune); 
#'     summary(survTune, metric = 'C-index');
#'     }
#' 
#' }
#'
#' @export
mpTune <- function(x, ...) UseMethod('mpTune');


#' @rdname mpTune
#' @export mpTune.formula
mpTune.formula <- function(formula, data, weights = NULL, ...) {
	mf <- model.frame(formula, data);
	x <- model.matrix(formula, data);
	y <- model.response(mf);
	xint <- match("(Intercept)", colnames(x), nomatch = 0);
	if (xint > 0) x <- x[, -xint, drop = FALSE];
	rm(mf);
	out <- mpTune.default(x, y, weights, ...);
	if(out$config$mpTnControl$returnData) {
		out$data <- list(formula = formula, data = data)
		}
	out$formula <- formula;
	class(out) <- c('mpTune.formula', class(out));
	return(out);
	}


#' @rdname mpTune
#' @export mpTune.default
mpTune.default <- function(
	x, y, 
	weights = NULL,
	models = list('rf', 'gbm'), 
	modelControl = list(),
	preProcess = NULL,
	gridLength = 5, 
	randomizedLength = 20, 
	mpTnControl = mpTuneControl(), 
	loopingRule = foreachLoop,
	verbose = TRUE, ...
	) {

	if(is.numeric(models)) {
		models <- getDefaultModel(models, y);
	}

	N.samples <- if(is.matrix(y) || is.data.frame(y)) nrow(y) else length(y);
	N.models <- length(models);
	gridLength <- rep(gridLength, length = N.models);
	randomizedLength <- rep(randomizedLength, length = N.models);
	lev <- if (is.factor(y)) levels(y) else NULL;

	perf.proto <- mpTnControl$summaryFunction(data.frame(
					   obs = y, 
					   pred = if(is.Surv(y)) y[, 1] else y
					   ));
	performance.names <- names(perf.proto); 

	internalModels <- !sapply(models, is.list, USE.NAMES = FALSE);
	if (is.null(names(models))) {
		namedModels <- rep(FALSE, length(models));
	} else {
		namedModels <- !sapply(names(models), `==`, '', USE.NAMES = FALSE);
		}
	if (!all(namedModels[!internalModels])) {
		stop("Customized models musted be named in 'models'!");
		}
	names(models)[!namedModels] <- models[!namedModels];

	modelInfo <- models;
	modelInfo[internalModels] <- sapply(models[internalModels], getModelInfo, regex = FALSE);
	models.input <- models;
	models <- names(models);

	models.tuneGrids <- makeTuneGrid(x, y, modelInfo, gridLength, randomizedLength);

	# create cv folds if not exist and check names
	if (!is.null(mpTnControl$sampleIndex)) {
		sampleIndex <- mpTnControl$sampleIndex;
	} else {
		sampleIndex <- mpTnControl$samplingFunction(x, y);
		}

	`%op%` <- if (mpTnControl$allowParallel) `%dopar%` else `%do%`;

	loopList <- makeloops(modelInfo = modelInfo, modelControl = modelControl,
					   models.tuneGrids = models.tuneGrids,
					   sampleIndex = sampleIndex);

	models.perf <- loopingRule(
		executeTask,
		loopList,
		x = x, y = y, weights = weights, 
		lev = lev, mpTnControl = mpTnControl, 
		preProcess = preProcess, perf.proto = perf.proto
		);

	# if (is.null(names(models.perf))) names(models.perf) <- names(loopList);
	modelLoopList <- sub('^(.*?)_\\._\\..*$', '\\1', names(models.perf));
	modelLoopList <- factor(modelLoopList, models);
	models.perf <- split(models.perf, modelLoopList);
	models.perf <- lapply(X = models.perf, FUN = function(z) {
					o <- do.call(rbind, z);
					rownames(o) <- NULL;
					return(o);
					})

	# average over resamples
	models.perf.reduced <- NULL;
	tryCatch({
		models.perf.reduced <- reducePerformance(
				models.perf, models.tuneGrids, 
				modelInfo, mpTnControl$allowParallel
				);
			}, 
		error = function(e) {print(e);}
		)

	return(structure(
		list(
			allModelsPerformance = models.perf.reduced,
			allCVs               = models.perf,
			data                 = if(mpTnControl$returnData) list(x = x, y = y, weights = weights) else NULL,
			performanceMetric    = names(perf.proto),
			config = list(
				sampleIndex      = sampleIndex,
				models           = models.input,
				modelControl     = modelControl,
				mpTnControl      = mpTnControl,
				preProcess       = preProcess,
				gridLength       = gridLength,
				randomizedLength = randomizedLength
				)
			), 
		class = 'mpTune'
		));
	}


executeTask <- function(
	x, y, weights, lev, 
	mpTnControl,
	preProcess,
	perf.proto,
	this.modelControl,
	this.model, this.info, this.tuneGrid, this.train,
	this.foldName, i.row
	) {

	# require(caret);
	if (interactive() && !mpTnControl$allowParallel) checkInstall(this.info$library);
	for (pkg in c(this.info$library)) {
		suppressPackageStartupMessages(
				do.call(require, list(pkg, quietly = TRUE, warn.conflicts = FALSE))
				)}

	thisRow.tuneGrid <- this.tuneGrid$loop[i.row, , drop = FALSE];
	this.submodels <- if(
			is.null(this.tuneGrid$submodels) || !nrow(this.tuneGrid$submodels[[i.row]])
			) NULL else this.tuneGrid$submodels[[i.row]];

		tryCatch(
			expr = {
				mod <- do.call(createModel,
					args = c(
						list(
							x          = x[this.train, , drop = FALSE],
							y          = y[this.train],
							wts        = if(is.null(weights)) NULL else weights[this.train],
							method     = this.info,
							tuneValue  = thisRow.tuneGrid,
							obsLevels  = lev,
							pp         = preProcess,
							classProbs = mpTnControl$classProbs
							),
						this.modelControl
						),
				quote = TRUE);

				fittingObjectSize <- object.size(mod);
				
				pred <- predictionFunction(
					method   = this.info,
					modelFit = mod$fit,
					newdata  = x[-this.train, ,drop = FALSE],
					preProc  = mod$preProc,
					param    = this.submodels
					);

				prob <- NULL;
				if (mpTnControl$classProbs && !is.null(this.info$prob) && !is.null(lev)) {

					prob <- probFunction(
						method   = this.info,
						modelFit = mod$fit,
						newdata  = x[-this.train, ,drop = FALSE],
						preProc  = mod$preProc,
						param    = this.submodels
						);
					}
				rm(mod);
				if (is.null(this.submodels)) {
					perf <- .getPerformanceSingle(
								summaryFunction = mpTnControl$summaryFunction, obs = y[-this.train], 
								pred = pred, prob = prob, lev = lev, model = this.model);
				} else {
					perf <- .getPerformanceList(
								summaryFunction = mpTnControl$summaryFunction, obs = y[-this.train], 
								pred = pred, prob = prob, lev = lev, model = this.model);
					}

				fitting.status <- 0;
				fitting.message <- '';

				runningNode <- Sys.info()['nodename']; # Sys.getpid();
				totalUsedMemory <- paste(round(sum(fittingObjectSize, 
												   sapply(ls(), function(x) object.size(get(x)))) / 1024^2, 3), 'Mb'); 
				out <- data.frame(
								  node = unname(runningNode), 
								  size = totalUsedMemory, 
								  status = fitting.status,
								  message = fitting.message, 
								  resample = this.foldName, 
								  expandParameters(thisRow.tuneGrid, this.submodels), 
								  perf, 
								  stringsAsFactors = FALSE,
								  check.names = FALSE);
				return(out);
				},
			error = function(e) {
				perf <- as.data.frame(matrix(NA_real_, nrow = 1, ncol = length(perf.proto)));
				names(perf) <- c(names(perf.proto));
				fitting.status <- 1;
				fitting.message <- paste(e, collapse = '\n'); #e$message;
				runningNode <- Sys.info()['nodename'];
				totalUsedMemory <- paste(round(sum(sapply(ls(), function(x) object.size(get(x)))) / 1024^2, 3), 'Mb'); 
				out <- data.frame(
								  node = unname(runningNode), 
								  size = totalUsedMemory, 
								  status = fitting.status,
								  message = fitting.message, 
								  resample = this.foldName, 
								  expandParameters(thisRow.tuneGrid, this.submodels), 
								  perf, 
								  stringsAsFactors = FALSE,
								  check.names = FALSE);
				return(out);
				}
			);
		}
